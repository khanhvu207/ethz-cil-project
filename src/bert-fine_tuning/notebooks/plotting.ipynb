{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_from_experiment(name):\n",
    "    mode = \"train\" if \"train\" in name else \"val\"\n",
    "    df = pd.read_csv(f\"../results/{name}.csv\")\n",
    "\n",
    "    steps = []\n",
    "    if \"val\" in name:\n",
    "        if \"dataset\" in name:\n",
    "            for i in range(len(df[\"trainer/global_step\"])):\n",
    "                if i % 2 == 0:\n",
    "                    steps.append(df[\"trainer/global_step\"][i])\n",
    "        else:\n",
    "            steps = df[\"trainer/global_step\"].tolist()\n",
    "        steps = steps[:19]\n",
    "    else:\n",
    "        steps = np.array(df[\"Step\"].tolist()[:3350]) * 50 + 49\n",
    "\n",
    "    def col_to_list(col_name):\n",
    "        res = [float(x) for x in df[col_name].tolist() if str(x) != \"nan\"]\n",
    "        if mode == \"val\":\n",
    "            res = res[:19]\n",
    "        else:\n",
    "            res = res[:3350]\n",
    "        return res\n",
    "\n",
    "    def ema(data, alpha=0.5):\n",
    "        if alpha == 0:\n",
    "            return data\n",
    "        ema = [data[0]]\n",
    "        for i in range(1, len(data)):\n",
    "            ema.append(alpha * data[i] + (1 - alpha) * ema[i - 1])\n",
    "        return ema\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    alpha = 0.99 if mode is \"train\" else 0.0\n",
    "\n",
    "    if \"dataset\" in name:\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"bert_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"BERT\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"bert_attention_cleaned - {mode}/accuracy\"), alpha),\n",
    "            label=\"BERT (cleaned)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(\n",
    "                col_to_list(f\"bert_attention_cleaned_lemmatized - {mode}/accuracy\"),\n",
    "                alpha,\n",
    "            ),\n",
    "            label=\"BERT (lemmatized)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(\n",
    "                col_to_list(\n",
    "                    f\"bert_attention_cleaned_spelling_corrected - {mode}/accuracy\"\n",
    "                ),\n",
    "                alpha,\n",
    "            ),\n",
    "            label=\"BERT (spelling corrected)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(\n",
    "                col_to_list(f\"bert_attention_stopword_removed - {mode}/accuracy\"), alpha\n",
    "            ),\n",
    "            label=\"BERT (stopword removed)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"bert_attention_hashtag - {mode}/accuracy\"), alpha),\n",
    "            label=\"BERT (hashtag)\",\n",
    "        )\n",
    "    else:\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"bert_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"BERT\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"roberta-base_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"RoBERTa (base)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"roberta-large_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"RoBERTa (large)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"timelm_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"TimeLM\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"albert-base_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"AlBERT (base)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"albert-large_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"AlBERT (large)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"albert-xlarge_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"AlBERT (xlarge)\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            steps,\n",
    "            ema(col_to_list(f\"deberta-large_attention - {mode}/accuracy\"), alpha),\n",
    "            label=\"DeBERTa (large)\",\n",
    "        )\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Number of training steps\", fontsize=11)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=11)\n",
    "    if mode == \"train\":\n",
    "        plt.title(\"Training accuracy\", fontsize=12)\n",
    "    else:\n",
    "        plt.title(\"Validation accuracy\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"../results/{name}.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot_from_experiment(\"bert-dataset-val_accuracy\")\n",
    "make_plot_from_experiment(\"bert-variant-val_accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
