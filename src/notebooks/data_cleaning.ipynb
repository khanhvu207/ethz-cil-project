{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "from textblob import TextBlob\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dknguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dknguyen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9<>\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=True,                     # lowercase text\n",
    "        no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=False,                  # replace all URLs with a special token\n",
    "        no_emails=False,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "        no_numbers=False,               # replace all numbers with a special token\n",
    "        no_digits=False,                # replace all digits with a special token\n",
    "        no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "        no_punct=True,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    return remove_non_alphanumeric(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/raw_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(labels=[\"length\"], axis=1, inplace=True)\n",
    "test_df.drop(labels=[\"length\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499995</th>\n",
       "      <td>a warning sign ? (; rt &lt;user&gt; the negativity y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499996</th>\n",
       "      <td>&lt;user&gt; ff too thank youuu ) )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499997</th>\n",
       "      <td>i just love shumpa ! that's my girl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499998</th>\n",
       "      <td>the best way to start a day ! no matter what h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499999</th>\n",
       "      <td>#frenchieswant1dtou i'm not from french but &lt;u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet  label\n",
       "0        vinco tresorpack 6 ( difficulty 10 of 10 objec...      0\n",
       "1        glad i dot have taks tomorrow ! ! #thankful #s...      0\n",
       "2        1-3 vs celtics in the regular season = were fu...      0\n",
       "3        <user> i could actually kill that girl i'm so ...      0\n",
       "4        <user> <user> <user> i find that very hard to ...      0\n",
       "...                                                    ...    ...\n",
       "2499995  a warning sign ? (; rt <user> the negativity y...      1\n",
       "2499996                      <user> ff too thank youuu ) )      1\n",
       "2499997                i just love shumpa ! that's my girl      1\n",
       "2499998  the best way to start a day ! no matter what h...      1\n",
       "2499999  #frenchieswant1dtou i'm not from french but <u...      1\n",
       "\n",
       "[2500000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i cant stay away from bug thats my baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>had a nice time w / my friend lastnite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>&lt;user&gt; no it's not ! please stop !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>not without my daughter ( dvd two-time oscar (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>&lt;user&gt; have fun in class sweetcheeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>making a r . e . a . l . difference . ( get r ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     sea doo pro sea scooter ( sports with the port...\n",
       "1     <user> shucks well i work all week so now i ca...\n",
       "2               i cant stay away from bug thats my baby\n",
       "3     <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "4     whenever i fall asleep watching the tv , i alw...\n",
       "...                                                 ...\n",
       "9995             had a nice time w / my friend lastnite\n",
       "9996                 <user> no it's not ! please stop !\n",
       "9997  not without my daughter ( dvd two-time oscar (...\n",
       "9998               <user> have fun in class sweetcheeks\n",
       "9999  making a r . e . a . l . difference . ( get r ...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplications and tweets that are longer than 140 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499995</th>\n",
       "      <td>a warning sign ? (; rt &lt;user&gt; the negativity y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499996</th>\n",
       "      <td>&lt;user&gt; ff too thank youuu ) )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499997</th>\n",
       "      <td>i just love shumpa ! that's my girl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499998</th>\n",
       "      <td>the best way to start a day ! no matter what h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499999</th>\n",
       "      <td>#frenchieswant1dtou i'm not from french but &lt;u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2260464 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     tweet  label\n",
       "0        vinco tresorpack 6 ( difficulty 10 of 10 objec...      0\n",
       "1        glad i dot have taks tomorrow ! ! #thankful #s...      0\n",
       "2        1-3 vs celtics in the regular season = were fu...      0\n",
       "3        <user> i could actually kill that girl i'm so ...      0\n",
       "4        <user> <user> <user> i find that very hard to ...      0\n",
       "...                                                    ...    ...\n",
       "2499995  a warning sign ? (; rt <user> the negativity y...      1\n",
       "2499996                      <user> ff too thank youuu ) )      1\n",
       "2499997                i just love shumpa ! that's my girl      1\n",
       "2499998  the best way to start a day ! no matter what h...      1\n",
       "2499999  #frenchieswant1dtou i'm not from french but <u...      1\n",
       "\n",
       "[2260464 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "train_df = train_df[train_df[\"tweet\"].str.len() <= 140]\n",
    "train_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, since we cannot drop anything, we just truncate the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tweet(t):\n",
    "    return t[:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"tweet\"] = test_df[\"tweet\"].apply(truncate_tweet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general function for transforming the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, funcs):\n",
    "    df_ = df.copy(deep=True)\n",
    "    tweets = df_[\"tweet\"]\n",
    "\n",
    "    for f in funcs:\n",
    "        tweets = Parallel(n_jobs=32)(delayed(f)(t) for t in tqdm(tweets))\n",
    "    \n",
    "    df_[\"tweet\"] = tweets\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "    'i',\n",
    "    'me',\n",
    "    'my',\n",
    "    'myself',\n",
    "    'we',\n",
    "    'our',\n",
    "    'ours',\n",
    "    'ourselves',\n",
    "    'you',\n",
    "    'your',\n",
    "    'yours',\n",
    "    'yourself',\n",
    "    'yourselves',\n",
    "    'he',\n",
    "    'him',\n",
    "    'his',\n",
    "    'himself',\n",
    "    'she',\n",
    "    'her',\n",
    "    'hers',\n",
    "    'herself',\n",
    "    'it',\n",
    "    'its',\n",
    "    'itself',\n",
    "    'they',\n",
    "    'them',\n",
    "    'their',\n",
    "    'theirs',\n",
    "    'themselves',\n",
    "    'what',\n",
    "    'which',\n",
    "    'who',\n",
    "    'whom',\n",
    "    'this',\n",
    "    'that',\n",
    "    'these',\n",
    "    'those',\n",
    "    'am',\n",
    "    'is',\n",
    "    'are',\n",
    "    'was',\n",
    "    'were',\n",
    "    'be',\n",
    "    'been',\n",
    "    'being',\n",
    "    'have',\n",
    "    'has',\n",
    "    'had',\n",
    "    'having',\n",
    "    'do',\n",
    "    'does',\n",
    "    'did',\n",
    "    'doing',\n",
    "    'a',\n",
    "    'an',\n",
    "    'the',\n",
    "    'and',\n",
    "    'but',\n",
    "    'if',\n",
    "    'or',\n",
    "    'because',\n",
    "    'as',\n",
    "    'until',\n",
    "    'while',\n",
    "    'of',\n",
    "    'at',\n",
    "    'by',\n",
    "    'for',\n",
    "    'with',\n",
    "    'about',\n",
    "    'against',\n",
    "    'between',\n",
    "    'into',\n",
    "    'through',\n",
    "    'during',\n",
    "    'before',\n",
    "    'after',\n",
    "    'above',\n",
    "    'below',\n",
    "    'to',\n",
    "    'from',\n",
    "    'up',\n",
    "    'down',\n",
    "    'in',\n",
    "    'out',\n",
    "    'on',\n",
    "    'off',\n",
    "    'over',\n",
    "    'under',\n",
    "    'again',\n",
    "    'further',\n",
    "    'then',\n",
    "    'once',\n",
    "    'here',\n",
    "    'there',\n",
    "    'when',\n",
    "    'where',\n",
    "    'why',\n",
    "    'how',\n",
    "    'all',\n",
    "    'any',\n",
    "    'both',\n",
    "    'each',\n",
    "    'few',\n",
    "    'more',\n",
    "    'most',\n",
    "    'other',\n",
    "    'some',\n",
    "    'such',\n",
    "    'no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'only',\n",
    "    'own',\n",
    "    'same',\n",
    "    'so',\n",
    "    'than',\n",
    "    'too',\n",
    "    'very',\n",
    "    'can',\n",
    "    'will',\n",
    "    'just',\n",
    "    'don',\n",
    "    'should',\n",
    "    'now'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "frequency_list = FreqDist(i.lower() for i in brown.words())\n",
    "common_english_words = []\n",
    "for word, freq in frequency_list.most_common()[:2000]:\n",
    "    if len(word) > 1:\n",
    "        common_english_words.append(word)\n",
    "\n",
    "def find_hashtags_and_split_words(text):\n",
    "    chunks = text.split(\" \")\n",
    "    result = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        if chunk[0] == \"#\":\n",
    "            result.append(\"#\")\n",
    "            current_word = \"\"\n",
    "            for c in chunk[1:]:\n",
    "                current_word += c\n",
    "                if current_word in stop_words or current_word in common_english_words:\n",
    "                    result.append(current_word)\n",
    "                    current_word = \"\"\n",
    "            if current_word != \"\":\n",
    "                result.append(current_word)\n",
    "        else:\n",
    "            result.append(chunk)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_hashtags_and_split_words(\"#shouldhavereaditwithaglassofwine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_train_df = transform(train_df, [find_hashtags_and_split_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_test_df = transform(test_df, [find_hashtags_and_split_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_train_df.to_csv(\"../data/hashtag_train.csv\", index=False)\n",
    "hashtag_test_df.to_csv(\"../data/hashtag_test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean raw tweets using clean-text library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2260464 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:30<00:00, 75254.06it/s] \n"
     ]
    }
   ],
   "source": [
    "cleaned_train_df = transform(train_df, [clean_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 32049.42it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_test_df = transform(test_df, [clean_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_df.to_csv(\"../data/cleaned_train.csv\", index=False)\n",
    "cleaned_test_df.to_csv(\"../data/cleaned_test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variants of the cleaned_train.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "    lemmatized_words = [\"<\"+word+\">\" if word in [\"user\", \"url\"] else word for word in lemmatized_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"<user>\"\n",
    "lemmatize_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12224/2260464 [00:00<00:49, 45277.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:24<00:00, 93745.85it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:46<00:00, 48834.55it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_lemmatized_train_df = transform(train_df, [clean_text, lemmatize_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 32227.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 34356.14it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_lemmatized_test_df = transform(test_df, [clean_text, lemmatize_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_lemmatized_train_df.to_csv(\"../data/cleaned_lemmatized_train.csv\", index=False)\n",
    "cleaned_lemmatized_test_df.to_csv(\"../data/cleaned_lemmatized_test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split(\" \") \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> went <url> see'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"<user> went to the <url> and see\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:28<00:00, 78500.24it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:21<00:00, 104912.73it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_stopword_removed_train_df = transform(train_df, [clean_text, remove_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 32125.32it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 42180.56it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_stopword_removed_test_df = transform(test_df, [clean_text, remove_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stopword_removed_train_df.to_csv(\"../data/cleaned_stopword_removed_train.csv\", index=False)\n",
    "cleaned_stopword_removed_test_df.to_csv(\"../data/cleaned_stopword_removed_test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_tweet = blob.correct()\n",
    "    return corrected_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"<user> went to home after the party\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spelling(\"<user> wnet to hoem after the patry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2260464/2260464 [00:28<00:00, 80388.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# cleaned_spelling_correction_train_df = transform(train_df, [clean_text, correct_spelling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_spelling_correction_test_df = transform(test_df, [clean_text, correct_spelling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_spelling_correction_train_df.to_csv(\"../data/cleaned_spelling_corrected_train.csv\", index=False)\n",
    "# cleaned_spelling_correction_test_df.to_csv(\"../data/cleaned_spelling_corrected_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
